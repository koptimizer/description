# 2020 빅콘테스트 챔피언리그: NS SHOP+ 판매실적 예측을 통한 편성 최적화 방안(모형) 도출
- 2020.07.20 ~ 2020.12.15

## 갑작스럽게 참여했던 컴페티션, 가장 많은 계단을 올랐던 경험
- 졸업 선배의 권유로 시작하게 되었던 데이터 분석 관련 대회였다. 지금 생각해보면 크게 준비가 되어있지 않았던 상태로 참여한 대회였기에 여러가지로 내 무지와 부족함에 답답함도 많이 느꼇던 것 같다. 
- 그럼에도 불구하고 심사위원분들이 좋게 봐주신 덕에 학부팀으로서 이렇게 큰 대회에서 우수상(6/1500+)이라는 쾌거를 달성했고, 이 좁은 바닥에서 내 선배가 되어주신 주옥같은 분들을 많이 만났던 시간이었다.
- 나는 발표와 EDA 및 최적화 모형에 대해서 담당했고, 특히 컴페티션 기간동안 최적화만 주구장창 팠던 기억이 난다. 애초에 나는 공고에 붙어있던 "최적화"라는 이 한 단어에 꽃혀서 이 컴페티션의 권유를 수락했던 것이기에 팀에 누가 되지 않도록 논문도 많이 읽고, 구현도 하루에 몇천 줄씩 했던 기억이 난다.  
- 평소에 메타휴리스틱에 관심이 많아서 구현도 많이 해보았고, 다양한 적용에 대한 논문도 많이 읽어왔던 터라 메타휴리스틱으로 접근을 했었는데, 이게 생각보다 평가에서 엄청난 어드벤티지를 받았던 것으로 생각된다. 대부분의 팀이 터키 알고리즘이나 EDA를 통해서 최적화를 했었는데 그러한 접근보다는 차원이 좀 더 높았다고 생각한다.

<그림 1.  공고 우수상 수상>

## EDA
- 나는 최적화 베이스 코드를 짜는 것을 주로 하고, EDA에서는 데이터 문해력이 좋은 팀원이 조달해준 시각화 과업이나 전처리를 처리해주는 것에 초점을 맞추었다.
- 솔직히 내가 EDA에 대해서 많이 감이 없었기에... 이 때에 어깨 너머로 많이 배웠다. 특히, 군집화를 통한 Feature Engineering이 어떻게 접근되어야 하는지에 대해서 많은 것들을 보고 배웠다. 
- 우리가 마주했던 가장 큰 문제는 Train Dataset에는 존재하지만 Test Dataset에는 존재하지 않는 마더코드와 상품명(상품코드)이 존재한다는 것이었는데, 이러한 것들을 Train Dataset과 매칭해주기 위해서 마더코드보다는 작고, 상품코드보다는 큰 단위의 중분류를 만들었다.
- 여기서 자연어 처리를 사용했는데, Konlpy를 이용해서 상품명을 토큰으로 잘라내고 W2V로 벡터화해서 각 토큰의 거리에 대한 연관 스코어 행렬을 구축하였다. 이렇게 하면 상품명이 비슷한 상품끼리 군집화가 되어 중분류 변수를 생성해줄 수 있다.
- 또, 선배가 특제 소스(?)라고 말하던 Jenson-Shannon Divergence 기반의 군집화도 진행했다. 비슷한 매출액 추이를 보이는 상품끼리 묶어서 변수를 생성한 것인데, 홈쇼핑의 특성상 비슷한 매출액을 보이는 제품들이 특정한 연령층이나 성별과 관련이 되기 때문에 모델링에서 해당 변수가 큰 효용을 보였다.
- 이외에도 Levenstein Distance를 이용해서 상품명의 특정 브랜드를 군집화 하거나 기후 상황과 같은 외부 데이터 셋도 사용했었는데, 외부 데이터 수집은 크롤링이 가능했던 내가 긁어다가 줬었다.
- 이렇게 대회 제출 약 이틀 전까지 거의 EDA만 주구장창 진행해서 초기 7개의 변수가 57개까지 부풀려졌었다. 최종적으로 csv로 변환된 파일을 눈으로 둘러보면서 이렇게 늘어난 변수에 뭔가 풍족한 느낌(?)을 받았는데, 이러한 느낌들이 압박감과 근심으로 거듭나기까지는 오래 걸리지 않았다...


## 모델링
- 대회 제출 이틀전 부터 숙소를 잡아서 팀원 전원이 크런치를 뛰었는데, 모델링이 대회 제출 4시간 전까지도 마무리가 안되어서(!!!) 최적화를 담당했던 나로서는 굉장히 난감했던 기억이 난다. 모델링이 되어야 최적화를 할 수 있기 때문...
- 모델링은 크게 별거 없었다. Gradeint Boosting이 해석하기에도 좋고 다중공선성 고려를 안해도 되니 GB의 프레임워크인 XGBoost와 LGBM을 사용했다. 다만, XGBoost가 Computational Time도 너무 길고 파라미터 튜닝하기에도 까다로워서 좀 더 가벼운 LGBM을 사용했다.
- 다행히 파라미터 최적화가 잘 마무리 되어서 대회 공식 지표인 MAPE도 잘 나왔기 때문에 최적화를 위한 실험시간 4시간(...)이 확보가 되었다. 내가 최적화를 돌리는 동안 나머지 팀원들이 변수 해석과 PT 제작을 맡아주었다. 
- 변수의 중요도를 해석하는 것은 당연하게도 SHAP Value와 Feature Importance를 사용했다. SHAP는 결과 지표의 관점에서 해당 변수가 얼마나 영향을 미치는지에 대한 지표이고, Feature Importance는 해당 변수가 트리의 분기에 얼마나 큰 관여를 하느냐에 대한 지표이다. 물론 둘 다 굉장히 중요한 지표이기 때문에 같이 언급하면 더 좋다.


## 최적화
- 내가 실질적으로 깊게 공부해봤던 건 최적화 밖에 없으니, 대회 시작 때부터 최적화 파트는 아예 맡겨달라고 자부했었다. 남들이 EDA와 모델링에 주요하게 시간을 투자할 때에 나는 메타휴리스틱 논문을 읽고 실험을 해보면서 베이스 코드를 만들어왔다.
- 메타휴리스틱으로 접근을 하는 이유는 내게 익숙한 것도 있었지만, 이외에도 여러 가지들이 있다.
  - 방송편성표를 최적화하는 것은 조합최적화 문제로 근사될 수 있으며, n개의 홈쇼핑 편성에 대한 조합의 경우의 수는 n!이다. 대회에서는 한 달짜리 2892개의 항목에 대한 방송편성표의 최적화를 요구했기 때문에 경우의 수는 2892!가 된다. 이미 우주의 원자의 개수의 제곱은 아득히 뛰어넘는 양이기 때문에 일반적인 수리최적화로는 접근할 수 없다.
  - 때문에 근사적 전역최적화 방법인 메타휴리스틱으로 접근하면 광활한 해 공간에서도 만족할 만한 해를 탐색할 수 있겠다고 생각했다. 물론 비교적 쉬운 구현은 덤이다.
  - "강화학습을 적용하면 더 효과적이지 않냐?" 라고 물어본다면 "매우 비효율적이다"라고 대답하겠다. 강화학습을 사용하는 것은 연속적인 의사결정상황, 빠른 속도의 필요성, 제약의 복잡성이 크지 않는(남은 시간이 많지 않기 때문에...) 등의 상황에서 사용되어야 하는 방법인데, 한 달짜리 방송편성표 최적화는 연속적인 의사 결정과 거리가 멀고, 속도 문제도 큰 이슈가 되지 못한다. 결정적으로 제약이 많이 존재하기 때문에 복잡성이 너무 높아서 구현에 들어가야하는 시간과 노력이 매우 크게 필요하다.

### Cuckoo Search
- 조합최적화 문제를 풀기위한 이산적 메타휴리스틱 알고리즘하면 Genetic Algorithm 아니겠는가? 그런데... 못썻다... 변수가 너무 많은 덕에 가뜩이나 느린 GA가 더 힘겨워 하는게 느껴졌기 때문. 추가적으로 Mutation 단에서 방송편성표라는 도메인의 제약이 너무 많았기 때문에 Feasible을 지키면서 해의 Mutation을 일으키는 것에 큰 어려움이 존재했다.
- 그래서 좀 더 가벼운 메타휴리스틱 알고리즘을 찾았지만, 조합최적화를 완벽히 수행할 수 있는 이산 메타휴리스틱 알고리즘은 거의 없었고, 있더라도 훨씬 더 복잡했다.
- 그래서 이미 존재하는 연속적 메타휴리스틱 알고리즘을 이산적으로 운용할 수 있는 방법을 모색했다.
- 결국 타겟으로 선정한 메타휴리스틱 알고리즘은 뻐꾸기 탐색법(Cuckoo Search)이었다. 뻐꾸기 탐색법은 뻐꾸기의 탁란(Brood Parasite)를 모사하여 전역 최적점을 찾아나가는 메타휴리스틱 알고리즘이다.
- 모든 메타휴리스틱 알고리즘은 탐색(Exploration)과 착취(Exploitation)을 적절히 수행해야 하는데, 뻐꾸기 탐색법은 레비 비행 분포(Levy Flights Distribution)의 멱법칙(Power Law)을 이용해서 탐색과 착취를 수행한다.
- 레비 비행 분포는 꼬리가 두꺼운 지수 함수의 형태를 띄는데 이러한 성질 때문에 확률 변수를 


## 프레젠테이션








