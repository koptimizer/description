# 소셜 감성 데이터를 이용한 가상화폐 가격 예측 모델
- 캡스톤 졸업 연구(진행중)
- 팀장

## 프로젝트 시작 계기: 마이너스의 손
- 어렸을 때부터 투자에 관심이 많았던지라 전역 이후 마땅한 수입이 생기는 시점부터 "최근까지" 펀드, 주식, 그리고 가상화폐에 투자를 했었다. 
- 최근까지인 이유는... 야수가 아닌 초식동물의 심장인 내가 항상 한 타임 늦은 투자를 했었고 손을 대는 족족 마이너스를 기록했기 때문에 이 서바이벌의 장에서 내 몫은 없다고 생각해 마음을 접었기 때문이다. 
- 물론 대책없이 투자했던 것은 아니다. 나름 경영학부 출신이니까 공시표도 한번 봐보고, 가상화폐 공개 깃헙도 분석해보고, 이름난 책도 몇 권 읽었던 것 같다.
- 그러다가 유독 우리나라에서 미친듯이 열광하던 가상화폐에 관심이 생겨서 가상화폐 관련 커뮤니티를 돌아보다가 그들이 게시판에 올린 각종 희노애락(오르면 웃고 내리면 우는...)을 보면서 그런 생각이 들었던 것 같다. "이걸로 비트코인의 가격을 예측해볼 수 있지 않을까?"

<p align = 'center'>
  <img src = "https://github.com/koptimizer/description/blob/main/note/projects/pics/soc1.jpg"><br/>
  (가상화폐 투자 관련 국내 최대 커뮤니티인 디씨인사이드 비트코인갤러리. 언제나 광기에 젖어있다...)
  </br>
</p>
<br/>

## 선행 연구 조사
- Pant DR et al(2018)는 트위터와 같은 소셜 네트워크 서비스를, 강민규 외(2020)에서는 비트코인 관련 뉴스를 수집하여 긍부정을 판별하고 Feature로 활용해서 비트코인의 가격을 예측하는 연구를 수행했다.
  - 두 연구 모두 당위성있는 접근이었기 때문에 본 프로젝트에서는 소셜 네트워크와 뉴스를 모두 활용하기로 하였다. 추가적으로, 투자자들의 희노애락을 가장 잘보여준다고 생각한 가상화폐 투자 관련 커뮤니티까지 선정하여 총 3개의 소스에서 데이터를 수집하기로 하였다.
  - 그러나 두 연구 모두 감성분석에서는 방법론적으로 조금 부족했다고 생각했다. 전자에서는 벡터화된 문장을 각종 분류기로 긍부정을 분류하여 보팅하였고, 후자에서는 TextBlob이라는 파이썬 라이브러리로 긍부정을 분류했다고 한다. TextBlob에 대해서 몇 가지 찾아본 결과 그냥 자연어 처리를 간단하게 할 수 있음에 중점이 맞춰진듯 보인다.
  - 보완점으로 BERT(Bidirectional Encoder Representations from Transformers)라는 매우 강력하고 최신의 언어 표현 모델을 활용하기로 결정하였다.
  - 커뮤니티 특유의 직설적이고 반어법이 자주 등장하는 표현의 감성 분석을 잘 수행하기 위해서는 BERT 특유의 학습법이 반드시 필요하다고 생각했기 때문이다.
- 또한, 이준식 외(2018)의 연구에서는 비트코인 가격변화에 영향을 주는 요인들에 대해서 소비자, 산업, 그리고 거시 변수를 중심으로 실증 분석하였는데, 해당 연구에서 유의미한 상관관계를 보인 가격결정 요인은 다음과 같다.
  - 비트코인 거래금지(Bitcoin Ban)에 대한 검색 트래픽
  - GPU 벤더 사의 주가 변화량
  - 달러 지수 변화량
  - 서부 텍사스 유가 변화량
- 타당성 있는 요인들이기 때문에 위 가격결정요인도 추가적인 Feature로 활용하기로 결정하였다. 그러나 검색 트래픽의 경우 본 연구에서는 이미 소셜 감성 데이터를 충분히 활용하고 있다고 판단해서 해당 요인은 제거하였다.

## BERT
- BERT는 Transformer에서 진보된 언어 표현 모델로써, Pre-training과 Fine-tunning을 통한 재활용성의 강점을 가지고 있다.
  - BERT는 Transformer와 비슷하지만, Encoder-Decoder의 형태가 아닌 Encoder로만 구성이 되어 있다는 게 가장 큰 차이점이다.
- BERT에 토크나이징된 문장이 입력되면 Token, Segment, Position이라는 세 가지의 임베딩을 사용해서 문장을 표현한다.
  - Token은 두 가지의 Special Token을 사용해서 문장의 시작 지점과 문장의 끝 지점에 임베딩
  - Segment는 문장을 구분
  - Position은 각 토큰의 위치를 표현
- BERT는 문장의 표현을 학습하기 위해 두 가지의 Unsupervised Learning을 사용한다.
  - Masked Language Model: 문장에서 단어의 일부를 Mask 토큰으로 가린 뒤, 가려진 단어를 예측하도록 학습한다. 이 과정에서 BERT는 문맥을 파악하는 능력을 학습.
  - Next Sentence Prediction: 문장 A와 B를 이어 붙인 후, 해당 관계가 올바른지 맞추도록 학습한다. 이 과정에서 BERT는 문장 사이의 관계를 학습.
- 이런 방식으로 학습된 BERT를 Fine-tunning할 때에 Class Label 개수만큼의 Layer를 붙여서 사용한다.

<p align = 'center'>
  <img src = "https://github.com/koptimizer/description/blob/main/note/projects/pics/soc2.jpg"><br/>
  (BERT의 Input Representation)
  </br>
</p>
<br/>

## 연구 방법
## 데이터 수집
- 가상화폐의 특징은 우선 장마감과 휴일이 없다는 것이다. 24시간 돌아가기 때문에 유의미한 예측을 하고자 한다면 최소 시간봉을 접근해야한다고 생각해서 가상화폐의 대표주자인 비트코인의 시간봉을 타겟으로 잡았다.
- 비트코인의 가격은 빗썸 기준으로 결정했다. 어느 정도 규모도 있고, 완전 과거 데이터까지 수집하려면 빗썸이 가장 편했기 때문...
- 이후 웹크롤링을 이용해서 비트코인과 관련된 트위터 게시글, 뉴스 기사, 관련 커뮤니티 게시글을 크롤링하기 위해 소스 사이트를 결정하였다.
  - 트위터 게시글 -> 트위터 내 "비트코인" 검색 결과 게시글들
  - 뉴스 기사 -> 가상화폐 관련 허브 디센터
  - 관련 커뮤니티 게시글 -> 디씨인사이드 비트코인 갤러리
- 위의 선행 연구에서 확보했던 가격결정요인 중, 비트코인 거래금지에 대한 검색 트래픽은 소셜 감성 데이터와 겹친다고 생각해서 이를 제외허고 수집하였다.
  - 모든 데이터는 야후 파이낸스에서 쉽게 구할 수 있었다. 다만 일일 데이터만 존재했기 때문에 이를 시간으로 확장시키고 같은 일에 해당하는 시간에 일일 가격을 삽입했다. 
- 이렇게 비트코인의 가격, 소셜 감성 데이터, 가격결정요인 3가지를 2020년 5월 15일부터 2021년 5월 31일까지의 데이터를 수집하였다.
  - 이는 디센터 뉴스가 2020년 5월 15일에 창간되었기 때문이다.

## 감성 분석
- 수집된 소셜 감성 데이터 내에서 의미 없는 문자열이나 광고를 정규표현식을 이용해서 삭제해주는 간단한 전처리 과정을 진행 후, Pretrained BERT로 긍부정 예측을 시도해보았다.
  - 사용된 Pretrained BERT는 구글에서 사전 학습시킨 모델에 네이터 영화 리뷰 코퍼스를 추가적으로 학습시킨 모델이다.
- 몇몇 테스트로 생성해본 문장에서는 잘 분류했지만, 내가 수집한 데이터들은 잘 분류해내지 못했다. 이러한 문제에 대해서 디씨인사이드라는 커뮤니티의 특성상 거친 언어와 직설적인 표현으로 BERT가 큰 혼동을 겪고 있다고 판단했다. 
  - 따라서 BERT에 이런 특징을 학습시키기 위해 원 데이터의 일부를 추출하고 직접 레이블링을 진행해서 추가 학습을 진행하기로 결정하였다.
  - 그렇게 총 2000개의 데이터를 직접 레이블링하여 준비하였고, 다시 예측을 시도해본 결과 성능이 70%대로 만족스럽지 못했다.
- 이러한 이유에 대해서 디센터의 뉴스 기사와 디씨인사이드의 게시글의 온도차가 크기 때문이라고 판단하였고 디센터와 디씨인사이드를 따로 분리해서 학습을 진행하였다. 트위터는 디씨인사이드와 비슷한 특성을 갖고 있다고 생각해서 디씨인사이드와 병합하였다.
  - 이렇게 학습을 진행하니 감성데이터에 대해서 소셜 커뮤니티쪽은 약 90%대의 분류율을, 뉴스쪽에서는 약 77%의 분류율을 달성 할 수 있었다.
  - 소셜쪽 분류율은 몹시 만족스러웠지만, 뉴스쪽은 비대칭성이 보이기도 하고... 성능도 조금 부족하다고 생각했지만... 길이가 하도 길어서 그런지 잘 학습이 되지 않아 마무리 지었다.
- 감성 분류 결과를 Feature로 활용하기 위해 시간단위로 사이트 별 긍정게시글의 갯수와 부정 게시글의 갯수를 Feature로 구현하였다.

<p align = 'center'>
  <img src = "https://github.com/koptimizer/description/blob/main/note/projects/pics/soc3.jpg"><br/>
  (A. 트위터와 비트코인 갤러리 데이터로 학습한 BERT의 Confusion Mat, B. 디센터로 학습한 BERT의 Confustion Mat)
  </br>
</p>
<br/>

## 최종 데이터 셋의 기본 형태
- 이렇게 완성된 데이터 셋은 아래와 같다. 시간봉이 종속 변수이고, C열부터 F열이 가격결정요인이다.
- G~L열은 각 사이트별 시간마다의 긍정 및 부정 게시글의 수이다.
- 시계열 모델 적용을 위해 종속 변수를 제외한 모든 변수에 Lack을 적용할 예정이다.

<p align = 'center'>
  <img src = "https://github.com/koptimizer/description/blob/main/note/projects/pics/soc4.jpg"><br/>
  (완성된 데이터 셋의 기본 형태)
  </br>
</p>
<br/>

## 모델링
- 데이터 셋이 완성이 되었으니 이제 모델링의 단계이다.
- 생각해둔 모델은 ARIMAX와 GRU, LGBM이다.
  - ARIMAX는 ARIMA의 다변수 예측 모델로 AR과 MA 기법을 이용한 가장 클래식한 시계열 예측모델이다.
  - GRU는 RNN 기반의 모델인 LSTM을 더욱 간단하게 구현한 모델로써 속도와 복잡성에서 이점을 가지고 있는 모델이다.
  - LGBM은 시계열모델은 아니지만, Boosting 기반의 방법이기 때문에 높은 예측력을 보장하고 Tree based model이 가진 다중공선성의 배제특성때문에 채택하였다. 충분한 Lack을 데이터에 부여할 것이기에 미약하게나마 시계열 효과를 볼 수 있을 것이라고 생각한다.

## 실험
- Soon...
